{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130e2c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.46.3)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.8/dist-packages (0.13.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (3.1.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.8/dist-packages (0.42.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.8/dist-packages (1.0.1)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.8/dist-packages (8.1.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from peft) (2.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from bitsandbytes) (1.10.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (8.12.3)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (3.0.14)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.13.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (3.0.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0; python_version < \"3.11\" in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.8/dist-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft datasets bitsandbytes accelerate ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf5acf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_neox.layers.0.attention.query_key_value\n",
      "gpt_neox.layers.0.attention.dense\n",
      "gpt_neox.layers.0.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.0.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.1.attention.query_key_value\n",
      "gpt_neox.layers.1.attention.dense\n",
      "gpt_neox.layers.1.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.1.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.2.attention.query_key_value\n",
      "gpt_neox.layers.2.attention.dense\n",
      "gpt_neox.layers.2.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.2.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.3.attention.query_key_value\n",
      "gpt_neox.layers.3.attention.dense\n",
      "gpt_neox.layers.3.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.3.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.4.attention.query_key_value\n",
      "gpt_neox.layers.4.attention.dense\n",
      "gpt_neox.layers.4.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.4.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.5.attention.query_key_value\n",
      "gpt_neox.layers.5.attention.dense\n",
      "gpt_neox.layers.5.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.5.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.6.attention.query_key_value\n",
      "gpt_neox.layers.6.attention.dense\n",
      "gpt_neox.layers.6.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.6.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.7.attention.query_key_value\n",
      "gpt_neox.layers.7.attention.dense\n",
      "gpt_neox.layers.7.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.7.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.8.attention.query_key_value\n",
      "gpt_neox.layers.8.attention.dense\n",
      "gpt_neox.layers.8.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.8.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.9.attention.query_key_value\n",
      "gpt_neox.layers.9.attention.dense\n",
      "gpt_neox.layers.9.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.9.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.10.attention.query_key_value\n",
      "gpt_neox.layers.10.attention.dense\n",
      "gpt_neox.layers.10.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.10.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.11.attention.query_key_value\n",
      "gpt_neox.layers.11.attention.dense\n",
      "gpt_neox.layers.11.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.11.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.12.attention.query_key_value\n",
      "gpt_neox.layers.12.attention.dense\n",
      "gpt_neox.layers.12.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.12.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.13.attention.query_key_value\n",
      "gpt_neox.layers.13.attention.dense\n",
      "gpt_neox.layers.13.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.13.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.14.attention.query_key_value\n",
      "gpt_neox.layers.14.attention.dense\n",
      "gpt_neox.layers.14.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.14.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.15.attention.query_key_value\n",
      "gpt_neox.layers.15.attention.dense\n",
      "gpt_neox.layers.15.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.15.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.16.attention.query_key_value\n",
      "gpt_neox.layers.16.attention.dense\n",
      "gpt_neox.layers.16.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.16.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.17.attention.query_key_value\n",
      "gpt_neox.layers.17.attention.dense\n",
      "gpt_neox.layers.17.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.17.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.18.attention.query_key_value\n",
      "gpt_neox.layers.18.attention.dense\n",
      "gpt_neox.layers.18.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.18.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.19.attention.query_key_value\n",
      "gpt_neox.layers.19.attention.dense\n",
      "gpt_neox.layers.19.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.19.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.20.attention.query_key_value\n",
      "gpt_neox.layers.20.attention.dense\n",
      "gpt_neox.layers.20.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.20.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.21.attention.query_key_value\n",
      "gpt_neox.layers.21.attention.dense\n",
      "gpt_neox.layers.21.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.21.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.22.attention.query_key_value\n",
      "gpt_neox.layers.22.attention.dense\n",
      "gpt_neox.layers.22.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.22.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.23.attention.query_key_value\n",
      "gpt_neox.layers.23.attention.dense\n",
      "gpt_neox.layers.23.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.23.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.24.attention.query_key_value\n",
      "gpt_neox.layers.24.attention.dense\n",
      "gpt_neox.layers.24.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.24.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.25.attention.query_key_value\n",
      "gpt_neox.layers.25.attention.dense\n",
      "gpt_neox.layers.25.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.25.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.26.attention.query_key_value\n",
      "gpt_neox.layers.26.attention.dense\n",
      "gpt_neox.layers.26.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.26.mlp.dense_4h_to_h\n",
      "gpt_neox.layers.27.attention.query_key_value\n",
      "gpt_neox.layers.27.attention.dense\n",
      "gpt_neox.layers.27.mlp.dense_h_to_4h\n",
      "gpt_neox.layers.27.mlp.dense_4h_to_h\n",
      "embed_out\n"
     ]
    }
   ],
   "source": [
    "# Ï∂úÎ†• Í∞úÏàò Ï†úÌïú + linear Í≥ÑÏ∏µÎßå ÌïÑÌÑ∞ÎßÅ\n",
    "for name, module in model.named_modules():\n",
    "    if \"linear\" in str(type(module)).lower():\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0e9c8",
   "metadata": {},
   "source": [
    "### [Step 1] Î™®Îç∏ Î°úÎìú Î∞è QloRA ÌÖåÏä§Ìä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b65bd01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3133e6997643d4bcd3b2eb5270fe41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14,680,064 || all params: 5,899,739,136 || trainable%: 0.2488\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "# 1. Î™®Îç∏ ÏßÄÏ†ï\n",
    "model_id = \"beomi/KoAlpaca-Polyglot-5.8B\"  # ÎòêÎäî 1.3B, 1.3BÎèÑ Ï∂©Î∂Ñ!\n",
    "\n",
    "# 2. ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä & Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞ (8bit Î°úÎî© + GPU ÏûêÎèô Ìï†Îãπ)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 3. LoRA ÌïôÏäµÏùÑ ÏúÑÌïú Î™®Îç∏ Ï§ÄÎπÑ\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 4. KoAlpaca Ï†ÑÏö© target_modulesÎ°ú LoRA ÏÑ§Ï†ï\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "# 5. LoRA Ï†ÅÏö©Îêú Î™®Îç∏ ÏÉùÏÑ±\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 6. ÌôïÏù∏Ïö© Ï∂úÎ†•\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4106e12",
   "metadata": {},
   "source": [
    "###  [Step 2] Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎî© Î∞è Ï†ÑÏ≤òÎ¶¨ ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738bf103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5e0f868ce74170ae5b37f23d7daae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e47ad431ca646758714d7eaf09747d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1. JSONL Î°úÎî©\n",
    "dataset = load_dataset(\"json\", data_files=\"/app/workspace/prompt_pairs.jsonl\", split=\"train\")\n",
    "\n",
    "# 2. Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò Ï†ïÏùò\n",
    "def format_prompt(example):\n",
    "    prompt = f\"### ÏßàÎ¨∏: {example['instruction']}\\n### ÎãµÎ≥Ä:\"\n",
    "    return tokenizer(prompt, text_target=example['output'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# 3. Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö©\n",
    "tokenized_dataset = dataset.map(format_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c23bb7",
   "metadata": {},
   "source": [
    "### [Step 3] Trainer Íµ¨ÏÑ± Î∞è ÌïôÏäµ Ïã§Ìñâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7838989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_23751/2917174663.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 02:38, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.146600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.916400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.885300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.595800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.873700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.717300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.960500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.338900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.417500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.862900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.208700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.698800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.783700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.679300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.742800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.331600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.631500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=2.776077366537518, metrics={'train_runtime': 163.5681, 'train_samples_per_second': 0.899, 'train_steps_per_second': 0.22, 'total_flos': 2555352667127808.0, 'train_loss': 2.776077366537518, 'epoch': 2.938775510204082})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# 1. ÌïôÏäµ ÏÑ§Ï†ï\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/app/workspace/qlora_outputs\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 2. Trainer Íµ¨ÏÑ±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "# 3. ÌïôÏäµ ÏãúÏûë\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc040b26",
   "metadata": {},
   "source": [
    "### [Step 4] ÌïôÏäµÎêú LoRA adapter Ï†ÄÏû•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb266ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/app/workspace/qlora_outputs/lora_adapter\")\n",
    "tokenizer.save_pretrained(\"/app/workspace/qlora_outputs/lora_adapter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ba2c1",
   "metadata": {},
   "source": [
    "### [Step 5] Ï∂îÎ°† ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c5cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b90fd43be414196ad63f11f52e1eb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. 8bit Î°úÎî© ÏÑ§Ï†ï\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # ÌïµÏã¨!\n",
    ")\n",
    "\n",
    "# 2. Base Î™®Îç∏ Î°úÎî©\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"beomi/KoAlpaca-Polyglot-5.8B\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# 3. LoRA adapter Î∂ôÏù¥Í∏∞\n",
    "model = PeftModel.from_pretrained(base_model, \"/app/workspace/qlora_outputs/lora_adapter\")\n",
    "model.eval()\n",
    "\n",
    "# 4. Tokenizer Î∂àÎü¨Ïò§Í∏∞\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/app/workspace/qlora_outputs/lora_adapter\", use_fast=False)\n",
    "\n",
    "#5. Ï∂îÎ°†Ìï®Ïàò\n",
    "def rewrite_prompt(input_ko):\n",
    "    prompt = f\"### ÏßàÎ¨∏: {input_ko}\\n### ÎãµÎ≥Ä:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    if 'token_type_ids' in inputs:\n",
    "        inputs.pop('token_type_ids')\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc0b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Í≤∞Í≥º:\n",
      " ### ÏßàÎ¨∏: Ï∞ΩÎ¨∏ ÎÑàÎ®∏Î•º Î∞îÎùºÎ≥¥Îäî Í≥†ÏñëÏù¥Î•º Í∑∏Î¶¨Í≥†Ïã∂ÏùÄÎç∞, ÎØ∏ÎìúÏ†ÄÎãà Í≥†ÎèÑÌôî ÌîÑÎ°¨ÌîÑÌä∏ ÏûëÏÑ±Ìï¥Ï§ò\n",
      "### ÎãµÎ≥Ä: Í≥†ÏñëÏù¥Î•º Í∑∏Î¶¨Î†§Î©¥ Í≥†ÏñëÏù¥Ïùò ÏãúÏÑ†Ï≤òÎ¶¨Î•º ÏúÑÌïú ÏÇ¨ÏßÑÏ≤©ÏùÑ ÎßåÎì§Ïñ¥Ï§ò\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 5. ÌÖåÏä§Ìä∏!\n",
    "example = \"Ï∞ΩÎ¨∏ ÎÑàÎ®∏Î•º Î∞îÎùºÎ≥¥Îäî Í≥†ÏñëÏù¥Î•º Í∑∏Î¶¨Í≥†Ïã∂ÏùÄÎç∞, ÎØ∏ÎìúÏ†ÄÎãà Í≥†ÎèÑÌôî ÌîÑÎ°¨ÌîÑÌä∏ ÏûëÏÑ±Ìï¥Ï§ò\"\n",
    "print(\"üí¨ Í≤∞Í≥º:\\n\", rewrite_prompt(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GPU ÏÇ¨Ïö© Í∞ÄÎä•Ïó¨Î∂Ä ÌôïÏù∏\n",
    "# import torch\n",
    "# print(torch.__version__)\n",
    "# print(\"GPU ÏÇ¨Ïö© Í∞ÄÎä•:\", torch.cuda.is_available())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
