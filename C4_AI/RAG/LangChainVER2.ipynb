{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# --- LangChain 및 관련 라이브러리 임포트 ---\n",
    "# Core\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.docstore.document import Document # 직접 Document 객체 생성 시\n",
    "\n",
    "# Memory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Loaders\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader, WikipediaLoader\n",
    "\n",
    "# Embeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Vectorstores\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# LLMs (HuggingFace Pipeline 사용 예시)\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "# Text Splitters\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Hugging Face 및 PyTorch 관련 라이브러리 임포트 ---\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "# 파인튜닝 모델 로딩 (qLoRA)에 필요할 수 있음\n",
    "try:\n",
    "    from peft import PeftModel행 전 확인 및 준비사항:\n",
    "\n",
    "경로 설정: 코드 상단의 FINETUNED_LLM_PATH, BASE_MODEL_NAME_OR_PATH, VECTOR_DB_PATH, WIKI_DATA_PATH, ADVANCED_PROMPTS_PATH 를 실제 환경에 맞게 정확히 수정하세요. 특히 파인튜닝된 어댑터 경로와 베이스 모델 이름이 중요합니다.\n",
    "라이브러리 설치: 필요한 모든 라이브러리가 설치되어 있는지 확인하세요. 특히 peft 라이브러리는 qLoRA 모델 로딩에 필수적일 수 있습니다.\n",
    "Bash\n",
    "\n",
    "pip install langchain langchain-community langchain-huggingface chromadb sentence-transformers torch transformers accelerate bitsandbytes pypdf wikipedia peft # 필요한 라이브러리 설치\n",
    "RAG 데이터 준비: WIKI_DATA_PATH와 ADVANCED_PROMPTS_PATH에 RAG에 사용할 실제 데이터 파일들을 위치시키세요. (위키 데이터는 예시 키워드 외에 프로젝트에 필요한 내용을 추가하거나, 직접 관련 텍스트 파일을 준비하는 것이 좋습니다.)\n",
    "GPU 메모리: 파인튜닝된 LLM을 로드하고 실행하기에 충분한 GPU VRAM이 필요합니다. 메모리가 부족하면 로딩에 실패하거나 매우 느릴 수 있습니다. (필요시 BitsAndBytesConfig를 사용한 양자화 설정을 조정하세요.)\n",
    "실행:\n",
    "\n",
    "위 코드를 conversational_rag.py 와 같은 이름으로 저장합니다.\n",
    "터미널에서 실행합니다: python conversational_rag.py\n",
    "실행되면 설정 정보와 함께 각 단계별 진행 상황이 출력됩니다.\n",
    "[User Input (Korean)]: 프롬프트가 나타나면 간단한 한국어 프롬프트를 입력하고 엔터를 누릅니다.\n",
    "시스템이 RAG 검색 및 LLM 추론을 통해 상세한 영어 프롬프트를 생성하여 출력합니다.\n",
    "이전 대화를 기억하므로, 다음 턴에는 \"거기에 파란색 모자를 추가해줘\" 와 같이 이전 프롬프트를 수정/보완하는 지시를 내릴 수 있습니다.\n",
    "exit를 입력하면 프로그램이 종료됩니다.\n",
    "\n",
    "    print(\"Warning: 'peft' library not found. pip install peft\")\n",
    "    PeftModel = None # peft 없으면 로딩 불가 처리\n",
    "\n",
    "# --- 1. 설정 ---\n",
    "# !!! 중요: 사용자의 환경에 맞게 경로와 모델 이름을 정확히 수정하세요 !!!\n",
    "FINETUNED_LLM_PATH = \"/path/to/your/finetuned-kanan-nano-2.1b-instruct/adapter\" # <<< 파인튜닝된 어댑터 가중치 경로\n",
    "BASE_MODEL_NAME_OR_PATH = \"NousResearch/Llama-2-7b-chat-hf\" # <<< 파인튜닝 시 사용한 베이스 모델 경로 또는 이름\n",
    "VECTOR_DB_PATH = \"./chroma_db_midjourney_rag_conv\" # Chroma DB 저장 경로\n",
    "EMBEDDING_MODEL_NAME = \"intfloat/e5-large-v2\" # 임베딩 모델 (다국어 지원)\n",
    "\n",
    "# RAG 소스 데이터 경로 설정 (예시)\n",
    "WIKI_DATA_PATH = \"./rag_data/wiki\"\n",
    "ADVANCED_PROMPTS_PATH = \"./rag_data/advanced_prompts\"\n",
    "os.makedirs(WIKI_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(ADVANCED_PROMPTS_PATH, exist_ok=True)\n",
    "\n",
    "# 예시 고급 프롬프트 데이터 (실제 데이터로 교체 필요)\n",
    "example_adv_prompt = \"\"\"\n",
    "masterpiece, best quality, ultra-detailed, illustration, beautiful detailed eyes, 1girl, cat ears, solo, long hair, blonde hair, blue eyes, school uniform, sailor collar, serafuku, pleated skirt, outdoors, sky, clouds, looking at viewer, dynamic angle, depth of field, cinematic lighting, volumetric light, detailed background of a vibrant cityscape at sunset --ar 16:9 --style raw --q 2\n",
    "\"\"\"\n",
    "with open(os.path.join(ADVANCED_PROMPTS_PATH, \"example_prompt.txt\"), \"w\", encoding='utf-8') as f:\n",
    "    f.write(example_adv_prompt)\n",
    "\n",
    "# 텍스트 분할 설정\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# 디바이스 설정 (GPU 우선 사용)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 대화 메모리 설정\n",
    "MEMORY_WINDOW_SIZE = 3 # 최근 몇 턴의 대화를 기억할지 설정\n",
    "\n",
    "# qLoRA 로딩을 위한 설정 (파인튜닝 시 사용한 설정과 일치시켜야 할 수 있음)\n",
    "# 예시: 4비트 로딩 설정 (필요시 주석 해제 및 조정, bitsandbytes 필요)\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16 # 또는 torch.float16\n",
    "# )\n",
    "\n",
    "print(f\"--- Configuration ---\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Embedding Model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"Base LLM for Finetune: {BASE_MODEL_NAME_OR_PATH}\")\n",
    "print(f\"Finetuned Adapter Path: {FINETUNED_LLM_PATH}\")\n",
    "print(f\"Vector DB Path: {VECTOR_DB_PATH}\")\n",
    "print(f\"Memory Window Size: {MEMORY_WINDOW_SIZE}\")\n",
    "print(f\"--------------------\")\n",
    "\n",
    "# --- 2. RAG용 데이터 로드 및 분할 ---\n",
    "print(\"\\n[Phase 1/7] Loading RAG source documents...\")\n",
    "all_docs = []\n",
    "\n",
    "# 2-1. 고급 프롬프트 예시 로드\n",
    "try:\n",
    "    prompt_loader = DirectoryLoader(ADVANCED_PROMPTS_PATH, glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True, loader_kwargs={'encoding': 'utf-8'})\n",
    "    advanced_prompt_docs = prompt_loader.load()\n",
    "    for doc in advanced_prompt_docs:\n",
    "        doc.metadata[\"source\"] = \"advanced_prompt_example\"\n",
    "    all_docs.extend(advanced_prompt_docs)\n",
    "    print(f\"- Loaded {len(advanced_prompt_docs)} advanced prompt examples.\")\n",
    "except Exception as e:\n",
    "    print(f\"- Error loading advanced prompts: {e}\")\n",
    "\n",
    "# 2-2. 위키피디아 정보 로드 (예시)\n",
    "wiki_keywords = [\"Cat\", \"Beach\", \"Forest\", \"Cyberpunk\", \"Steampunk\", \"Oil painting\", \"Illustration\", \"Photorealistic\"] # 예시 키워드\n",
    "wiki_docs_to_load = []\n",
    "print(f\"- Loading Wikipedia data for keywords (max 1 doc per keyword): {wiki_keywords}...\")\n",
    "try:\n",
    "    for keyword in wiki_keywords:\n",
    "        # WikipediaLoader는 영어 위키 기준, 한국어 필요 시 다른 방식 고려\n",
    "        loader = WikipediaLoader(query=keyword, load_max_docs=1, doc_content_chars_max=2000) # 내용 길이 제한\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = f\"wikipedia_{keyword}\"\n",
    "        wiki_docs_to_load.extend(docs)\n",
    "    all_docs.extend(wiki_docs_to_load)\n",
    "    print(f\"- Loaded {len(wiki_docs_to_load)} documents from Wikipedia.\")\n",
    "except Exception as e:\n",
    "    print(f\"- Error loading Wikipedia data: {e}. Skipping Wikipedia loading.\")\n",
    "    print(\"- Consider manually adding relevant Wikipedia text files.\")\n",
    "\n",
    "# 2-3. 문서 분할\n",
    "if not all_docs:\n",
    "    print(\"\\nError: No documents loaded for RAG. Please check data paths and loaders. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n- Total documents loaded: {len(all_docs)}\")\n",
    "print(\"- Splitting documents...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "texts = text_splitter.split_documents(all_docs)\n",
    "print(f\"- Split into {len(texts)} text chunks.\")\n",
    "\n",
    "# --- 3. 임베딩 모델 로드 ---\n",
    "print(\"\\n[Phase 2/7] Initializing embedding model...\")\n",
    "try:\n",
    "    model_kwargs = {'device': DEVICE}\n",
    "    encode_kwargs = {'normalize_embeddings': True} # E5 모델 권장 사항\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    print(\"- Embedding model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError initializing embedding model: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. 벡터 DB 생성 및 저장 (Chroma) ---\n",
    "print(\"\\n[Phase 3/7] Initializing Chroma vector store...\")\n",
    "try:\n",
    "    if os.path.exists(VECTOR_DB_PATH):\n",
    "        print(f\"- Loading existing vector store from {VECTOR_DB_PATH}\")\n",
    "        vectordb = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)\n",
    "    else:\n",
    "        print(f\"- Creating new vector store at {VECTOR_DB_PATH} (this may take a while)...\")\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=texts,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=VECTOR_DB_PATH\n",
    "        )\n",
    "        vectordb.persist()\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 3}) # 관련성 높은 청크 3개 검색\n",
    "    print(\"- Vector store initialized and retriever created.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError initializing vector store: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- 5. 파인튜닝된 LLM 로드 ---\n",
    "# !!! 중요: qLoRA 등 파인튜닝 방식에 맞춰 정확한 로딩 코드 필요 !!!\n",
    "print(f\"\\n[Phase 4/7] Initializing Finetuned LLM...\")\n",
    "llm = None\n",
    "try:\n",
    "    if not PeftModel:\n",
    "        raise ImportError(\"PeftModel is not available. Cannot load LoRA adapter.\")\n",
    "\n",
    "    # 1. 베이스 모델 로드\n",
    "    print(f\"- Loading base model: {BASE_MODEL_NAME_OR_PATH}\")\n",
    "    # qLoRA 사용 시 bitsandbytes 설정 (bnb_config) 필요할 수 있음\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME_OR_PATH,\n",
    "        # quantization_config=bnb_config, # 4비트 로딩 설정 적용 시\n",
    "        torch_dtype=torch.bfloat16 if DEVICE == \"cuda\" else torch.float32, # GPU 환경에 맞게 조정\n",
    "        device_map=\"auto\" # 자동 디바이스 매핑 (GPU 활용)\n",
    "    )\n",
    "    print(\"- Base model loaded.\")\n",
    "\n",
    "    # 2. 어댑터(파인튜닝 가중치) 로드 및 병합 (Peft 사용)\n",
    "    print(f\"- Loading adapter (finetuned weights) from: {FINETUNED_LLM_PATH}\")\n",
    "    model = PeftModel.from_pretrained(model, FINETUNED_LLM_PATH)\n",
    "    # 필요하다면 모델 병합: model = model.merge_and_unload() (병합 후에는 어댑터 경로 불필요)\n",
    "    print(\"- Adapter loaded.\")\n",
    "\n",
    "    # 3. 토크나이저 로드 (베이스 모델의 토크나이저 사용)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME_OR_PATH)\n",
    "\n",
    "    # 4. LangChain 파이프라인 생성\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256, # 생성할 최대 토큰 수 (미드저니 프롬프트 길이 고려)\n",
    "        temperature=0.7,    # 다양성\n",
    "        top_p=0.95,         # 샘플링 확률\n",
    "        repetition_penalty=1.1 # 반복 방지\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    print(\"- Finetuned LLM loaded successfully via HuggingFacePipeline.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--------------------------------------------------------------\")\n",
    "    print(f\"Error loading finetuned LLM: {e}\")\n",
    "    print(f\"Please check:\")\n",
    "    print(f\"  1. Correctness of paths: '{BASE_MODEL_NAME_OR_PATH}' and '{FINETUNED_LLM_PATH}'\")\n",
    "    print(f\"  2. Availability of 'peft' library and its compatibility.\")\n",
    "    print(f\"  3. Sufficient GPU memory and required dependencies (transformers, accelerate, bitsandbytes).\")\n",
    "    print(f\"Exiting application as LLM is not available.\")\n",
    "    print(f\"--------------------------------------------------------------\")\n",
    "    exit()\n",
    "\n",
    "# --- 6. 메모리 및 RAG 체인 구성 ---\n",
    "print(\"\\n[Phase 5/7] Configuring Conversational RAG chain...\")\n",
    "\n",
    "# 6-1. 메모리 초기화\n",
    "try:\n",
    "    memory = ConversationBufferWindowMemory(\n",
    "        k=MEMORY_WINDOW_SIZE, # 기억할 턴 수\n",
    "        memory_key=\"chat_history\", # 프롬프트 템플릿과 일치\n",
    "        return_messages=True # ChatPromptTemplate과 호환\n",
    "    )\n",
    "    print(f\"- Conversation memory initialized (Window size: {MEMORY_WINDOW_SIZE})\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError initializing memory: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# 6-2. 프롬프트 템플릿 정의\n",
    "try:\n",
    "    conversational_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert prompt engineer specializing in Midjourney. Your task is to translate and refine a simple Korean prompt into a highly detailed English prompt, considering the ongoing conversation history and provided context. Base your response primarily on the user's latest instruction, using history and context for enhancement.\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), # 대화 기록 삽입 위치\n",
    "        (\"human\", \"\"\"**Context from Knowledge Base & Examples (based on latest instruction):**\n",
    "{context}\n",
    "\n",
    "**User's Latest Instruction (Korean):**\n",
    "{korean_input}\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the conversation history (`chat_history`) to understand the progression.\n",
    "2. Focus on the `User's Latest Instruction (Korean)` for the main goal of this turn.\n",
    "3. Use the provided `Context` (retrieved based on the latest instruction) to add relevant details, styles, camera angles, lighting etc.\n",
    "4. Generate an updated or refined English Midjourney prompt based on the latest instruction and context, considering the history.\n",
    "5. Output ONLY the final English prompt string for this turn, without any extra explanation or conversation.\n",
    "\n",
    "**Refined English Midjourney Prompt:**\n",
    "\"\"\"),\n",
    "    ])\n",
    "    print(\"- Conversational prompt template created.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError creating prompt template: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# 6-3. 헬퍼 함수: 검색된 문서 포맷팅\n",
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        content_preview = doc.page_content[:200].replace(\"\\n\", \" \") # 미리보기, 줄바꿈 제거\n",
    "        formatted_docs.append(f\"--- Context Source [{source}] ---\\n{content_preview}...\")\n",
    "    return \"\\n\\n\".join(formatted_docs) if formatted_docs else \"No relevant context found.\"\n",
    "\n",
    "# 6-4. 핵심 RAG 체인 정의 (LCEL)\n",
    "# 메모리 로드/저장은 이 체인 외부 루프에서 처리\n",
    "try:\n",
    "    core_rag_chain = (\n",
    "        RunnableParallel(\n",
    "            # context는 현재 korean_input 기반으로 검색 후 포맷팅\n",
    "            context=RunnableLambda(lambda x: retriever.invoke(x[\"korean_input\"])) | format_docs,\n",
    "            # korean_input과 chat_history는 그대로 전달 (이후 단계에서 추출)\n",
    "            passthrough=RunnablePassthrough()\n",
    "        )\n",
    "        | RunnableLambda(lambda x: { # 프롬프트 템플릿에 필요한 변수만 정확히 매핑\n",
    "              \"context\": x[\"context\"],\n",
    "              \"korean_input\": x[\"passthrough\"][\"korean_input\"], # passthrough 딕셔너리에서 값 추출\n",
    "              \"chat_history\": x[\"passthrough\"][\"chat_history\"] # passthrough 딕셔너리에서 값 추출\n",
    "          })\n",
    "        | conversational_prompt_template # 프롬프트 적용\n",
    "        | llm # LLM 호출\n",
    "        | StrOutputParser() # 출력 파싱 (문자열)\n",
    "    )\n",
    "    print(\"- Conversational RAG chain configured successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError configuring RAG chain: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 7. 대화형 프롬프트 변환 실행 ---\n",
    "print(\"\\n[Phase 6/7] Ready for Conversational Prompt Transformation.\")\n",
    "print(\"Enter your simple Korean prompt. Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        korean_input = input(\"\\n[User Input (Korean)]: \")\n",
    "        if korean_input.lower() == 'exit':\n",
    "            break\n",
    "        if not korean_input.strip():\n",
    "            continue\n",
    "\n",
    "        print(\"\\n[System] Processing...\")\n",
    "\n",
    "        # 1. 메모리에서 이전 대화 기록 로드\n",
    "        loaded_memory = memory.load_memory_variables({})\n",
    "        chat_history = loaded_memory.get('chat_history', []) # memory_key와 일치\n",
    "\n",
    "        # 2. 핵심 RAG 체인 호출 (현재 입력 + 로드된 기록 전달)\n",
    "        inputs = {\"korean_input\": korean_input, \"chat_history\": chat_history}\n",
    "        english_prompt_output = core_rag_chain.invoke(inputs)\n",
    "\n",
    "        # 3. 현재 턴의 입력과 출력을 메모리에 저장\n",
    "        memory.save_context({\"input\": korean_input}, {\"output\": english_prompt_output})\n",
    "\n",
    "        print(\"\\n--- Generated English Midjourney Prompt ---\")\n",
    "        print(english_prompt_output)\n",
    "        print(\"-------------------------------------------\")\n",
    "\n",
    "        # (디버깅용) 검색된 컨텍스트 확인\n",
    "        # retrieved_docs = retriever.invoke(korean_input)\n",
    "        # print(\"\\n[Debug] Retrieved Context:\")\n",
    "        # print(format_docs(retrieved_docs))\n",
    "        # print(\"-------------------------\")\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n[System] Interrupted by user. Exiting.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[Error] An error occurred during processing: {e}\")\n",
    "        # 오류 발생 시 다음 루프 계속 진행\n",
    "        continue\n",
    "\n",
    "print(\"\\n[Phase 7/7] Exiting application.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4daa3c",
   "metadata": {},
   "source": [
    "실실실행 전 확인 및 준비사항:\n",
    "\n",
    "경로 설정: 코드 상단의 FINETUNED_LLM_PATH, BASE_MODEL_NAME_OR_PATH, VECTOR_DB_PATH, WIKI_DATA_PATH, ADVANCED_PROMPTS_PATH 를 실제 환경에 맞게 정확히 수정하세요. 특히 파인튜닝된 어댑터 경로와 베이스 모델 이름이 중요합니다.\n",
    "라이브러리 설치: 필요한 모든 라이브러리가 설치되어 있는지 확인하세요. 특히 peft 라이브러리는 qLoRA 모델 로딩에 필수적일 수 있습니다.\n",
    "Bash\n",
    "\n",
    "pip install langchain langchain-community langchain-huggingface chromadb sentence-transformers torch transformers accelerate bitsandbytes pypdf wikipedia peft # 필요한 라이브러리 설치\n",
    "RAG 데이터 준비: WIKI_DATA_PATH와 ADVANCED_PROMPTS_PATH에 RAG에 사용할 실제 데이터 파일들을 위치시키세요. (위키 데이터는 예시 키워드 외에 프로젝트에 필요한 내용을 추가하거나, 직접 관련 텍스트 파일을 준비하는 것이 좋습니다.)\n",
    "GPU 메모리: 파인튜닝된 LLM을 로드하고 실행하기에 충분한 GPU VRAM이 필요합니다. 메모리가 부족하면 로딩에 실패하거나 매우 느릴 수 있습니다. (필요시 BitsAndBytesConfig를 사용한 양자화 설정을 조정하세요.)\n",
    "실행:\n",
    "\n",
    "1. 위 코드를 conversational_rag.py 와 같은 이름으로 저장합니다.\n",
    "2. 터미널에서 실행합니다: python conversational_rag.py\n",
    "3. 실행되면 설정 정보와 함께 각 단계별 진행 상황이 출력됩니다.\n",
    "4. [User Input (Korean)]: 프롬프트가 나타나면 간단한 한국어 프롬프트를 입력하고 엔터를 누릅니다.\n",
    "5. 시스템이 RAG 검색 및 LLM 추론을 통해 상세한 영어 프롬프트를 생성하여 출력합니다.\n",
    "6. 이전 대화를 기억하므로, 다음 턴에는 \"거기에 파란색 모자를 추가해줘\" 와 같이 이전 프롬프트를 수정/보완하는 지시를 내릴 수 있습니다.\n",
    "7. exit를 입력하면 프로그램이 종료됩니다.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
